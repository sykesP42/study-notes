# 📦 压缩（Compression）—— 从信息论到霍夫曼编码，深入数据压缩原理

> 本文是 CS61B Spring 2024 Lecture 38 的超详细笔记，全面讲解数据压缩的基本概念、信息论基础、变长编码、香农-范诺编码、霍夫曼编码的实现与理论极限。通过大量示例、数学推导、代码思路和思考题，帮助你深入理解压缩的本质，并为后续学习更高级的压缩算法（如 LZW、JPEG）打下基础。

---

## 🔍 一、引言：压缩实例与基本概念

### 1.1 文件压缩示例：《白鲸记》

我们从一个实际例子开始：使用 `zip` 命令压缩赫尔曼·梅尔维尔的经典小说《白鲸记》（mobydick.txt），原始大小为 **643 KB**，压缩后为 **261 KB**，压缩率约为 **59%**。

- **无损压缩**：解压后可以完全还原原始文件，即存在一个压缩函数 `C` 和对应的解压函数 `C⁻¹`，使得 `C⁻¹(C(B)) = B` 对于所有可能的输入 `B` 成立。
    
- **单射性**：压缩函数必须是单射的（injective），即不同的原始数据必须映射到不同的压缩数据，否则解压时会产生歧义。
    

### 1.2 压缩的三大目标

1. **信息传递**：压缩后的数据应携带与原始数据相同的信息量。
    
2. **存储效率**：减少存储空间，例如将 643 KB 的文件存储为 261 KB。
    
3. **传输效率**：在网络传输中，压缩可以显著减少带宽占用。
    

### 1.3 为什么文本可以压缩？

因为自然语言（如英语）存在大量**冗余**和**可预测性**。例如，字母 `e` 的出现频率远高于 `z`，单词 `the` 频繁出现。压缩算法利用这些统计特性，用更短的编码表示高频符号，从而降低平均码长。

---

## 📊 二、信息论基础：信息量与熵

### 2.1 信息量的直观理解：记忆难度实验

为了理解“信息量”，我们可以做一个简单的心理实验：尝试记忆不同字符串的难度。

|字符串示例|长度|记忆难度（1-10）|说明|
|---|---|---|---|
|`BXX ONHP WTP`|10字符|2-3|完全随机字母，难以找到规律|
|`ONE A POEM A RAVEN MIDNIGHTS SO DREARY TIRED AND WEARY`|47字符|约2-3|来自诗歌《乌鸦》，有语义和结构，虽然长度是前者的4.7倍，但记忆难度相当|
|`AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA`|10000字符|1|全部相同，极易记忆|

**结论**：数据的信息量与它的**可预测性**成反比。越可预测的数据，实际携带的信息量越少，越容易被压缩。

### 2.2 比特与字符表示

在计算机中，信息的基本单位是**比特（bit）**，即一个二进制位（0 或 1）。通常，文本用 **8 位（1 字节）** 表示一个字符，例如 ASCII 码中字母 `'d'` 表示为 `01100100`（十六进制 `64`）。

但实际存储的信息量可能远小于 8 比特/字符。例如，英语文本的每个字符平均只携带约 **1 比特** 的信息（后面会解释）。

### 2.3 香农熵（Shannon Entropy）

**香农熵** 是信息论中衡量数据不确定性的指标。对于一个随机变量 `X`，取值为 `x₁, x₂, …, xₙ`，对应概率 `p₁, p₂, …, pₙ`，其熵定义为：

text

H(X) = - Σ pᵢ log₂ pᵢ   （单位：比特）

熵表示**平均每个符号所需的最小编码长度**。

**示例**：假设一个只有两个字符的字母表 `{A, B}`，出现概率分别为 0.5 和 0.5，则熵为 `-0.5 log₂0.5 -0.5 log₂0.5 = 1` 比特/符号。如果 `A` 概率 0.9，`B` 概率 0.1，则熵为 `-0.9 log₂0.9 -0.1 log₂0.1 ≈ 0.47` 比特/符号，说明可预测性高，可以编码得更短。

### 2.4 英语的熵：约 1 比特/字符

通过实验（如 Shannon 1950 年的论文），发现英语文本的熵约为 **1 比特/字符**。这意味着，理论上最优压缩算法可以将 47 个字符的英语句子压缩到 47 比特（约 6 字节），压缩率高达 87%！但实际上，由于复杂性和计算成本，我们很难达到这个极限。

仅考虑字母频率时，英语的熵约为 4.1 比特/字符；考虑单词频率可降至 2.6 比特/字符；考虑更长的上下文可进一步降低。但 Shannon 的实验表明，人类利用上下文预测，最终熵约为 1 比特/字符。

---

## 🔣 三、变长编码与前缀码

### 3.1 固定长度编码的局限性

- **ASCII**：每个字符固定 8 位，简单但浪费空间。
    
- **布莱叶盲文**：用 2×3 的点阵表示字母，每个点对应 1 比特（凸起/平坦），共 6 比特，可表示 64 种符号，足够覆盖 26 个字母和标点。
    
- **问题**：4 比特只能表示 16 种符号，不足以覆盖 26 个字母。
    

解决方案：使用**变长编码**，高频符号用短码，低频用长码，从而降低平均长度。

### 3.2 莫尔斯电码（Morse Code）

莫尔斯电码是最著名的变长编码：

- 点（dot）= 0，划（dash）= 1。
    
- 字母编码长度 1~4 比特，例如 `E` = `.` (0)，`T` = `-` (1)，`A` = `.-` (01)，`Z` = `--..` (1100)。
    

**优点**：常用字母（E、T）用最短编码，平均长度较短。

**缺点**：**前缀歧义**——某些编码是其他编码的前缀。例如 `E` 的编码 `0` 是 `A` 的编码 `01` 的前缀。在连续的比特流中，如果没有分隔符，就无法唯一解码。例如 `001` 可以解读为 `E E T`（0 0 1）或 `A T`（01 1）？实际莫尔斯电码中通过**停顿**（时间间隔）来分隔字母，但在纯二进制流中无法使用停顿。

### 3.3 前缀码（Prefix-Free Code）

为了避免歧义，我们需要**前缀码**：没有任何一个编码是另一个编码的前缀。这种编码可以保证唯一解码，无需分隔符。

前缀码可以用**二叉树**表示：

- 每个叶子节点对应一个符号。
    
- 左分支代表 0，右分支代表 1。
    
- 从根到叶子的路径即为该符号的编码。
    

例如，一个简单的前缀码：

text

符号： A   B   C   D
编码： 0   10  110 111

对应的树：

text

        root
       /    \
      0      1
     (A)    / \
           10 11
          (B) / \
             110 111
             (C) (D)

任何符号的编码都不会是另一个的前缀，因为每个符号对应一个叶子节点。

---

## 📐 四、香农-范诺编码（Shannon-Fano Coding）

香农-范诺编码是一种构造近似最优前缀码的算法，由 Claude Shannon 和 Robert Fano 提出。它不是最优的，但为霍夫曼编码奠定了基础。

### 4.1 算法步骤

1. 统计每个符号的频率（或概率）。
    
2. 将符号按频率降序排序。
    
3. 递归地将符号列表分成两部分，使得两部分的频率之和尽可能接近。
    
4. 左半部分编码以 0 开头，右半部分以 1 开头。
    
5. 对每一半递归执行步骤 3-4，直到每个部分只有一个符号。
    

### 4.2 示例

假设有 5 个符号，频率如下：

|符号|频率|
|---|---|
|三|0.35|
|点|0.17|
|一|0.17|
|四|0.16|
|四|0.15|

（注意：这里的“四”出现了两次？实际上可能是两个不同的符号，但频率接近，我们当作两个独立符号处理。）

按频率排序后：三(0.35), 点(0.17), 一(0.17), 四(0.16), 四(0.15)。

**第一次分割**：总和 1.0，前半部分尽量接近 0.5。尝试将前三项（三+点+一）加起来 0.35+0.17+0.17 = 0.69，太大大；前两项 0.35+0.17=0.52，接近 0.5，所以前半部分取“三”和“点”（总和 0.52），后半部分取“一”、“四”、“四”（总和 0.48）。前半部分编码前缀 0，后半部分前缀 1。

**第二次分割**：

- 前半部分（三、点）：0.35 和 0.17，总和 0.52。分割：单独取“三”作为左子（0.35），编码 00；右子“点”编码 01。
    
- 后半部分（一、四、四）：0.17+0.16+0.15=0.48。再分割：前两项（一+四=0.33）和后一项（四=0.15），前项前缀 10，后项前缀 11。
    
    - 前项（一、四）：0.17 和 0.16，再分割：左子“一”编码 100，右子“四”编码 101。
        
    - 后项（最后一个四）编码 110。
        

最终编码：

- 三：00
    
- 点：01
    
- 一：100
    
- 四：101
    
- 四：110
    

平均码长 = 0.35×2 + 0.17×2 + 0.17×3 + 0.16×3 + 0.15×3 = 0.7 + 0.34 + 0.51 + 0.48 + 0.45 = 2.48 比特/符号。

### 4.3 非最优性

香农-范诺编码不保证最优。有时通过不同的分割方式可以得到更短的编码。霍夫曼编码能保证平均码长最小。

---

## 🌳 五、霍夫曼编码（Huffman Coding）

霍夫曼编码由 David A. Huffman 于 1952 年提出，是一种**最优前缀码**，即对于给定的频率分布，它产生的平均码长最小。

### 5.1 算法思想（贪心）

1. 为每个符号创建一个叶子节点，节点权重为符号的频率。
    
2. 重复以下步骤直到只剩一棵树：
    
    - 选择权重最小的两个节点（可以是叶子或内部节点）合并为一个新节点，新节点的权重为两者之和。
        
    - 新节点的左子节点为其中一个，右子节点为另一个（顺序可任意，但通常左小右大）。
        
3. 最终得到的二叉树即为霍夫曼树。从根到每个叶子的路径（左 0 右 1）给出该符号的编码。
    

### 5.2 详细示例（使用上述频率）

符号及频率：

- 三：0.35
    
- 点：0.17
    
- 一：0.17
    
- 四：0.16
    
- 四：0.15
    

**步骤**：

1. 初始有 5 个节点：三(0.35), 点(0.17), 一(0.17), 四(0.16), 四(0.15)。
    
2. 找两个最小：0.15 和 0.16（两个“四”），合并为节点 A(0.31)。
    
3. 现有节点：三(0.35), 点(0.17), 一(0.17), A(0.31)。
    
4. 找两个最小：点(0.17) 和一(0.17)，合并为节点 B(0.34)。
    
5. 现有：三(0.35), A(0.31), B(0.34)。
    
6. 找两个最小：A(0.31) 和 三(0.35)？实际上 0.31 和 0.34？0.31 < 0.34，所以最小两个是 A(0.31) 和 三(0.35)？但 0.34 介于之间？需要重新排序：当前权重：0.31(A), 0.34(B), 0.35(三)。最小两个是 A(0.31) 和 B(0.34)？0.31+0.34=0.65，比 0.31+0.35=0.66 小，所以应合并 A 和 B？但 A 和 B 都是内部节点，可以合并。合并 A 和 B 得节点 C(0.65)。
    
7. 现有：三(0.35) 和 C(0.65)。
    
8. 合并为根节点 D(1.0)。
    

树结构：

text

        D(1.0)
       /    \
   C(0.65)  三(0.35)   [这里将三放在右边，左边是 C]
   /    \
A(0.31) B(0.34)
/ \     / \
0.15 0.16 点 一
       (0.17)(0.17)

注意：在合并时，我们需记录哪个是左子哪个是右子，这会影响编码，但不影响平均码长（因为左右可互换，只要保持一致）。

**分配编码**：

- 从根到三：先右（1）→ 三的编码为 `1`。
    
- 从根到 A：先左（0），再到 A 的左子（0）得 `00`，对应两个“四”？我们需要区分两个“四”。实际上 A 的两个子分别是频率 0.15 和 0.16 的“四”，我们可称它们为“四1”和“四2”。它们的编码分别是 `000` 和 `001`（取决于左右）。
    
- 从根到 B：先左（0）再右（1），然后 B 的左子是点（0），右子是一（1），所以点的编码为 `010`，一的编码为 `011`。
    

因此最终编码：

- 三：1（1 位）
    
- 点：010（3 位）
    
- 一：011（3 位）
    
- 四1：000（3 位）
    
- 四2：001（3 位）
    

平均码长 = 0.35×1 + 0.17×3 + 0.17×3 + 0.16×3 + 0.15×3 = 0.35 + 0.51 + 0.51 + 0.48 + 0.45 = 2.30 比特/符号。

### 5.3 与香农-范诺编码对比

香农-范诺编码平均码长为 2.48，霍夫曼为 2.30，明显更优。理论上，霍夫曼编码达到最优，即对于给定频率，没有任何前缀码能获得更小的平均码长。

### 5.4 最优性证明（简要）

霍夫曼编码的最优性基于贪心选择性质：通过合并最小权重的节点，可以保证最终树的加权路径长度最小。可以用数学归纳法证明：假设对所有小于 n 个符号的情况成立，则对于 n 个符号，将两个最小频率合并后，问题规模减小，且得到的树加上新节点后仍最优。

---

## 💻 六、霍夫曼编码的实现

### 6.1 数据结构：优先队列（最小堆）

为了高效找到最小权重的两个节点，我们使用**优先队列**（最小堆）。每个节点可以是叶子节点（包含字符和频率）或内部节点（包含频率和左右子指针）。

**节点类设计**（Java 风格）：



```java

class HuffmanNode implements Comparable<HuffmanNode> {
    char ch;          // 字符（仅叶子节点有效）
    int freq;         // 频率
    HuffmanNode left, right;
    
    public HuffmanNode(char ch, int freq) {
        this.ch = ch;
        this.freq = freq;
    }
    
    public HuffmanNode(int freq, HuffmanNode left, HuffmanNode right) {
        this.freq = freq;
        this.left = left;
        this.right = right;
    }
    
    public boolean isLeaf() {
        return left == null && right == null;
    }
    
    @Override
    public int compareTo(HuffmanNode other) {
        return this.freq - other.freq;
    }
}
```
### 6.2 构建霍夫曼树

```java

public static HuffmanNode buildHuffmanTree(Map<Character, Integer> freqMap) {
    PriorityQueue<HuffmanNode> pq = new PriorityQueue<>();
    for (Map.Entry<Character, Integer> entry : freqMap.entrySet()) {
        pq.add(new HuffmanNode(entry.getKey(), entry.getValue()));
    }
    while (pq.size() > 1) {
        HuffmanNode left = pq.poll();
        HuffmanNode right = pq.poll();
        HuffmanNode parent = new HuffmanNode(left.freq + right.freq, left, right);
        pq.add(parent);
    }
    return pq.poll();
}
```
### 6.3 生成编码表

通过深度优先遍历树，记录路径（左 0 右 1），得到每个字符的编码。

```java

public static void buildCodeMap(HuffmanNode root, String code, Map<Character, String> codeMap) {
    if (root.isLeaf()) {
        codeMap.put(root.ch, code);
        return;
    }
    buildCodeMap(root.left, code + "0", codeMap);
    buildCodeMap(root.right, code + "1", codeMap);
}
```
### 6.4 压缩过程

压缩一个文件需要两个部分：

1. **存储霍夫曼树**（解码时需要知道树结构）。通常将树以某种格式写入文件头部，例如使用前序遍历，用特殊标记区分叶子节点。
    
2. **存储编码后的数据**：将原始文件每个字符替换为对应的编码比特流，并拼成一个连续的比特序列。
    

由于 Java 的 `BitSet` 或直接按位操作可以处理比特流。实际实现中，常将编码写入字节数组，最后补零。

**注意**：需要额外存储树的长度或结束标记，以便解压时知道何时停止。

### 6.5 解压过程

1. 从文件头部读取并重建霍夫曼树。
    
2. 读取编码后的比特流。
    
3. 从根开始，根据每个比特（0 左，1 右）向下走，直到到达叶子节点，输出该字符，然后回到根继续。
    

```java

public static void decompress(HuffmanNode root, BitInputStream in, OutputStream out) {
    HuffmanNode node = root;
    while (in.hasBits()) {
        int bit = in.readBit();
        if (bit == 0) node = node.left;
        else node = node.right;
        if (node.isLeaf()) {
            out.write(node.ch);
            node = root;
        }
    }
}
```
---

## 🏭 七、实际应用中的霍夫曼编码

### 7.1 静态 vs 动态霍夫曼编码

- **静态编码**：使用固定的频率表（例如基于大量英语文本统计），对所有文件使用同一编码。优点是无需在压缩文件中存储树，缺点是对于特定文件可能不是最优。
    
- **动态编码**：为每个文件单独统计频率并构建树，需要将树存储在文件头部。对于大文件，树的大小（几十字节）可以忽略不计；对于小文件，树可能比节省的空间还大，因此通常用于大文件。
    

### 7.2 存储树的格式

一种简单方法：用前序遍历，每个内部节点输出一个特殊标记（如 '0'），叶子节点输出 '1' 后跟字符的 ASCII 码。解压时根据标记重建。

例如，对于上例的树，前序遍历可能为：内部节点、内部节点、叶子（四1）、叶子（四2）、内部节点、叶子（点）、叶子（一）、叶子（三）。具体实现需设计协议。

### 7.3 霍夫曼编码在流行算法中的应用

- **ZIP**：使用 LZ77 和霍夫曼编码的组合。
    
- **JPEG**：对 DCT 系数使用霍夫曼编码。
    
- **PNG**：使用 LZ77 和霍夫曼编码（DEFLATE 算法）。
    

---

## 🔧 八、其他压缩方法简介

### 8.1 游程编码（Run-Length Encoding, RLE）

适用于连续重复数据。例如，字符串 `XXXXXYYYZZZZ` 可编码为 `X5Y3Z4`。但需要小心处理数字和字符的混淆，通常用特殊标记。

RLE 在图像压缩（如 BMP 格式）中常用。

### 8.2 LZW 算法（Lempel-Ziv-Welch）

LZW 是一种字典压缩算法，它动态建立字典，将重复出现的子串替换为字典索引。常用于 GIF 图像格式。其核心思想是：一边压缩一边构建字典，解压时也能重建相同的字典。

---

## 🧮 九、压缩理论极限

### 9.1 鸽巢原理：不能普遍压缩 50%

假设有一个声称能**压缩任何比特流 50%** 的算法 `SuperZip`。考虑所有长度为 1000 的比特流，共有 2¹⁰⁰⁰ 种可能。压缩后长度 ≤ 500 比特，而 ≤ 500 比特的比特流总数最多为 2⁵⁰¹ - 1（包括从 0 到 500 位的所有可能，但 0 位也算一种）。显然 2¹⁰⁰⁰ 远大于 2⁵⁰¹，因此必然有多个不同的原始比特流映射到同一个压缩结果，导致无法唯一解压。因此，不存在可以压缩所有输入的通用压缩算法。

### 9.2 香农熵作为下界

对于给定的概率分布，任何无损压缩算法的平均码长都不能低于香农熵。这是信息论的基本定理。换句话说，**你无法压缩随机数据**（因为随机数据的熵接近最大）。

### 9.3 随机数据的不可压缩性

如果输入是均匀随机的比特串，那么每个符号的概率相等，熵为 1 比特/符号，任何压缩算法都会使其平均长度至少为 1 比特/符号（可能还会因存储开销而变长）。实际上，如果考虑压缩算法本身作为数据的一部分，对于随机输入，压缩后的平均长度反而会**增加**（因为要存储解码信息）。

### 9.4 与 P=NP 问题的关联

找到一个能对任意数据达到最优压缩（即最小化平均码长）的通用算法是极其困难的。事实上，如果存在一个多项式时间的算法能够解决“最优压缩”问题（即给定数据，找到最短的可能编码），那么它将能够解决许多 NP 问题，从而证明 P=NP。这正是千禧年难题之一。目前普遍认为 P≠NP，因此不存在这样的通用最优压缩算法。

---

## 📋 十、知识点总结（彩色标记）

|知识点|核心内容|<span style="color:red">重点/易错点</span>|
|---|---|---|
|无损压缩定义|压缩函数必须是单射，解压能完全还原|<span style="color:red">不同输入必须映射到不同输出，否则无法解压</span>|
|香农熵|衡量信息量的指标，H = -Σ p log₂ p|<span style="color:red">熵是压缩的理论下界，无法突破</span>|
|英语熵|约 1 比特/字符|<span style="color:red">比 ASCII 的 8 比特/字符低得多，说明英语冗余大</span>|
|前缀码|没有任何编码是另一个的前缀|<span style="color:red">可用二叉树表示，保证唯一解码</span>|
|莫尔斯电码|变长编码，但非前缀码，需分隔符|<span style="color:red">前缀问题导致歧义</span>|
|香农-范诺编码|递归分割频率表，生成前缀码|<span style="color:red">不保证最优，但直观</span>|
|霍夫曼编码|贪心合并最小频率节点，生成最优前缀码|<span style="color:red">平均码长最小，且可通过优先队列高效实现</span>|
|霍夫曼树构建|使用最小堆反复合并最小节点|<span style="color:red">合并后的节点继续参与比较，直到只剩一个根</span>|
|平均码长计算|Σ (频率 × 码长)|<span style="color:red">与熵对比可评估编码效率</span>|
|压缩极限|鸽巢原理：不能压缩所有输入|<span style="color:red">任何算法都会使某些输入变长</span>|
|与 P=NP 的关系|最优压缩算法若存在多项式时间，则 P=NP|<span style="color:red">尚未解决，普遍认为 P≠NP</span>|

---

## 🧠 十一、思考题与答案

### 1. 为什么莫尔斯电码需要加入停顿符才能唯一解码？

**答案**：莫尔斯电码不是前缀码，例如 `E` 的编码 `.` 是 `A` 的编码 `.-` 的前缀。在连续比特流中，如果没有分隔符，无法区分 `.` 和 `.-`。停顿符相当于隐式分隔符，告诉接收者一个字母结束。

### 2. 假设有 5 个符号，频率分别为 0.4, 0.2, 0.2, 0.1, 0.1。请构造霍夫曼树并计算平均码长。

**答案**：

- 合并两个 0.1 → 节点 A(0.2)
    
- 现有：0.4, 0.2, 0.2, A(0.2)
    
- 合并两个 0.2（例如 0.2 和 0.2）→ 节点 B(0.4)
    
- 现有：0.4, B(0.4), A(0.2)
    
- 合并 A(0.2) 和 0.4（例如 0.2 和 0.4）→ 节点 C(0.6)
    
- 现有：0.4, C(0.6)
    
- 合并得根 D(1.0)
    

分配编码（假设左 0 右 1）：

- 0.4 的符号（假设为 a）可能编码为 0（若在左）或 1，取决于合并顺序。这里假设最终树：
    
    text
    
          D
         / \
        C   a(0.4)
       / \
      B  0.2?
     / \
    

0.1 0.1

```text

等等，需要具体画出树。为了准确，我们按合并顺序记录：
第一次合并两个 0.1 为 A(0.2)。
第二次合并两个 0.2（其中一个原始 0.2 和另一个原始 0.2）为 B(0.4)。
第三次合并 A(0.2) 和原始 0.2（剩下的那个 0.2）为 C(0.4)。
第四次合并 C(0.4) 和原始 0.4 为根。
但这样会有两个 0.4 的节点，可能最后合并的是 C(0.4) 和原始 0.4。编码可能：
- 原始 0.4（假设符号 a）：如果它在最后一步是右子，则编码为 1。
- C(0.4) 在左子，编码 0。C 的左子 B(0.4) 和右子原始 0.2？我们需要追踪。
更清晰的步骤：
初始：n1(0.4), n2(0.2), n3(0.2), n4(0.1), n5(0.1)
合并 n4,n5 → A(0.2)
现有：n1(0.4), n2(0.2), n3(0.2), A(0.2)
合并 n2 和 n3 → B(0.4)
现有：n1(0.4), B(0.4), A(0.2)
合并 A 和 n1 → C(0.6) [假设 A 左，n1 右]
现有：B(0.4), C(0.6)
合并 B 和 C → 根，假设 B 左，C 右
则树：
根
├─ B(左)
│  ├─ n2(左) -> 编码 00
│  └─ n3(右) -> 编码 01
└─ C(右)
 ├─ A(左)
 │  ├─ n4(左) -> 编码 100
 │  └─ n5(右) -> 编码 101
 └─ n1(右) -> 编码 11
 
```
所以编码：
- n1(0.4): 11 (2 位)
- n2(0.2): 00 (2 位)
- n3(0.2): 01 (2 位)
- n4(0.1): 100 (3 位)
- n5(0.1): 101 (3 位)
平均码长 = 0.4×2 + 0.2×2 + 0.2×2 + 0.1×3 + 0.1×3 = 0.8 + 0.4 + 0.4 + 0.3 + 0.3 = 2.2 比特/符号。
### 3. 为什么不可能存在一个能压缩所有 1000 位字符串到 500 位的算法？
**答案**：根据鸽巢原理，1000 位字符串有 2¹⁰⁰⁰ 种，而 0-500 位的字符串总数最多为 2⁵⁰¹ - 1（因为 0 位到 500 位，每个长度最多 2^长度种，总和小于 2⁵⁰¹）。由于 2¹⁰⁰⁰ 远大于 2⁵⁰¹，必然存在两个不同的 1000 位字符串映射到同一个压缩结果，导致无法唯一解压。因此这样的算法不可能存在。
### 4. 假设一段文本由 5 种字符组成，频率分别为 0.35, 0.17, 0.17, 0.16, 0.15。请用霍夫曼编码计算平均码长，并与香农熵比较。
**答案**：我们在示例中已计算霍夫曼平均码长为 2.30 比特/符号。该分布的香农熵为：
H = -0.35 log₂0.35 -0.17 log₂0.17 -0.17 log₂0.17 -0.16 log₂0.16 -0.15 log₂0.15
计算：
0.35 log₂0.35 ≈ 0.35 × (-1.5146) = -0.5301
0.17 log₂0.17 ≈ 0.17 × (-2.556) = -0.4345（两次）
0.16 log₂0.16 ≈ 0.16 × (-2.6439) = -0.4230
0.15 log₂0.15 ≈ 0.15 × (-2.737) = -0.4106
总和 ≈ - (0.5301+0.4345+0.4345+0.4230+0.4106) = -2.2327，所以 H ≈ 2.23 比特/符号。霍夫曼平均码长 2.30 略高于熵，这是因为熵是理论下界，霍夫曼编码只能逼近但通常不能完全达到（除非频率是 2 的幂）。这里 2.30 > 2.23，符合预期。
### 5. 在霍夫曼编码中，如果两个字符频率相同，合并顺序会影响最终编码吗？会影响平均码长吗？
**答案**：合并顺序会影响具体编码（哪个字符得 0，哪个得 1），但不会影响平均码长。因为交换左右子树相当于互换编码，但码长不变。因此任意合并顺序都能得到相同的最优平均码长（可能因树结构不同导致个别符号码长变化，但平均相同）。
### 6. 霍夫曼编码的解压过程如何知道比特流何时结束？
**答案**：通常压缩文件会额外存储原始文件的长度，或者在编码比特流末尾添加一个特殊的结束标记（例如在霍夫曼树中不存在的符号）。解压时，当输出字符数达到原始长度时停止，或者遇到结束标记。
### 7. 如果一个文件的所有字符频率相同，霍夫曼编码的压缩效果如何？
**答案**：如果所有字符等概率，那么每个字符的熵为 log₂N 比特（N 为字符种类数）。霍夫曼编码会为每个字符分配几乎等长的码（可能因合并过程导致一些长度差 1），平均码长接近 log₂N，但可能比固定长度编码（如 ceil(log₂N) 位）稍差或相等。实际上，对于等概率分布，固定长度编码可能更简单且最优（如果 N 是 2 的幂，则固定长度就是最优；否则霍夫曼可能稍好一点，但接近）。
### 8. 为什么说“任何压缩算法都会使某些输入变长”？
**答案**：因为压缩函数是单射，输入空间大小大于输出空间大小（对于固定长度输出）。为了给所有可能的输入分配唯一的输出，必须有些输入映射到较长的输出。例如，将 1000 位字符串映射到 0-999 位，必然存在许多字符串被映射到比原长度更长的结果，否则无法容纳所有输入。
### 9. 游程编码适用于什么类型的数据？举例说明。
**答案**：游程编码适用于有连续重复符号的数据，例如简单图形（大面积相同颜色）、传真图像、某些文本（如包含连续空格）。例如，字符串 `"AAABBBCCCC"` 可编码为 `"A3B3C4"`。如果数据中很少有连续重复，RLE 反而会使数据膨胀（例如 `"ABC"` 可能变成 `"A1B1C1"`，更长）。
### 10. 如果你要设计一个压缩算法，如何权衡压缩率与速度？
 **答案**：通常压缩率越高的算法越复杂，耗时越多。例如，霍夫曼编码需要统计频率和建树，比简单 RLE 慢，但压缩效果好。在实际应用中，需要根据需求选择：对于实时传输（如视频会议），可能使用快速算法（如某些 LZ 变体）；对于存档（如备份），可能使用高压缩率但慢的算法（如 LZMA）。另外，解压速度通常比压缩更重要，因为文件往往只压缩一次，但解压多次。
 
---
